# ğŸŒ™ Astra AI â€“ Multimodal Conversational Assistant

Astra is a **Flask-based AI assistant** powered by **Ollama (LLaMA 3)** for local text generation, **gTTS** for speech synthesis, and **D-ID** for avatar video generation.  
It supports **three modes of response**:  
1. ğŸ“ **Text**  
2. ğŸ”Š **Audio**  
3. ğŸ¥ **Video with talking avatar**  

---

## âš¡ Features
- Local **chat response** via [Ollama](https://ollama.com/) (LLaMA 3 model).  
- **Text-to-Speech** using [gTTS](https://pypi.org/project/gTTS/).  
- **Talking avatar video** generation with [D-ID API](https://d-id.com/).  
- Simple **Flask web UI** with real-time responses.  
- Organized project structure with static assets and templates.  

---

## ğŸ“‚ Project Structure
```
ASTRA_AI-PROJECT/
â”‚â”€â”€ app.py                # Flask backend (Ollama + gTTS + D-ID)
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ README.md              # Project documentation
â”‚â”€â”€ .gitignore             # Ignore venv, cache, media files
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/               # Stylesheets
â”‚   â”œâ”€â”€ js/                # Frontend JS
â”‚   â”œâ”€â”€ img/               # UI images
â”‚   â”œâ”€â”€ audio/             # Generated speech files
â”‚   â””â”€â”€ video/             # Generated video files
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Frontend UI
â”‚
â””â”€â”€ output/                # Optional outputs
```

---

## ğŸ”§ Installation & Setup

### 1. Clone repo
```bash
git clone https://github.com/yourusername/astra-ai.git
cd astra-ai
```

### 2. Create & activate virtual environment
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux / macOS
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Install & run Ollama
- [Download Ollama](https://ollama.com/download)  
- Pull LLaMA 3 model:
```bash
ollama pull llama3
```
- Run Ollama server:
```bash
ollama serve
```

### 5. Set up environment variables
Create a `.env` file:
```ini
DID_API_KEY=your_did_api_key   # Base64 encoded Basic Auth key
```

On Windows (PowerShell):
```powershell
setx DID_API_KEY "your_base64_basic_auth_value"
```

On Linux/macOS:
```bash
export DID_API_KEY="your_base64_basic_auth_value"
```

---

## â–¶ï¸ Run the app
```bash
python app.py
```

App will be available at: **http://localhost:5000/**

---

## ğŸ’¡ Usage
- Enter your **prompt** in the UI.  
- Select a response **mode**:
  - `Text` â†’ plain response.  
  - `Audio` â†’ response with generated MP3 speech.  
  - `Video` â†’ D-ID generated talking avatar.  

---

## ğŸ“¦ Requirements
- Python 3.8+  
- Flask  
- gTTS  
- Requests  
- Ollama (with LLaMA 3 pulled)  
- D-ID API key (for video mode)  

---

## ğŸš€ Future Improvements
- Replace D-ID with **local lip-sync (Wav2Lip)**.  
- Add **multimodal input (voice prompts, image understanding)**.  
- Deploy on **Docker** for portability.  

---

## ğŸ“ License
MIT License. Free to use & modify.  
